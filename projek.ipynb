import os
import random
import numpy as np
import pandas as pd

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import tensorflow as tf
from tensorflow.keras import layers

# =========================
# 0) Reproducibility
# =========================
def set_seed(seed: int = 42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

# =========================
# 0b) CONFIG (tuning knobs)
# =========================
CSV_PATH = "/content/pekanbaru_clean_final.csv"

LOOKBACK = 120
LR = 8e-4
DROPOUT = 0.05           # << lebih rendah agar tidak terlalu smooth
EPOCHS = 200
BATCH_SIZE = 32

# Weighted loss settings (naikkan WEIGHT_K kalau puncak masih kurang tinggi)
LOSS_MODE = "weighted_huber"   # "weighted_mse" / "weighted_huber"
WEIGHT_K = 6.0
HUBER_DELTA = 0.02

DO_ENSEMBLE = True
ENSEMBLE_SEEDS = [7, 42, 202]

# =========================
# 1) Load & preprocess
# =========================
df = pd.read_csv(CSV_PATH)
df["TANGGAL"] = pd.to_datetime(df["TANGGAL"])
df = df.sort_values("TANGGAL").reset_index(drop=True)

# Target: 30-day rolling mean
df["RR_30d_mean"] = df["RR"].rolling(30).mean()

# Seasonal encoding (month)
df["month"] = df["TANGGAL"].dt.month
df["month_sin"] = np.sin(2*np.pi*df["month"]/12)
df["month_cos"] = np.cos(2*np.pi*df["month"]/12)

# Long lags of rainfall (memory effect)
df["RR_lag30"] = df["RR"].shift(30)
df["RR_lag60"] = df["RR"].shift(60)
df["RR_lag90"] = df["RR"].shift(90)

# Drop NA from rolling/lag
df = df.dropna().reset_index(drop=True)

# =========================
# 2) Features & target
# =========================
feature_cols = [
    "TN", "TX", "TAVG", "RH_AVG", "SS",
    "FF_X", "DDD_X", "FF_AVG", "DDD_CAR",
    "month_sin", "month_cos",
    "RR_lag30", "RR_lag60", "RR_lag90"
]
target_col = "RR_30d_mean"

X_all = df[feature_cols].values.astype(np.float32)
y_all = df[target_col].values.astype(np.float32)

# =========================
# 3) Chronological split (no shuffle)
# =========================
n = len(df)
train_end = int(n * 0.70)
val_end   = int(n * 0.85)

X_train, y_train = X_all[:train_end], y_all[:train_end]
X_val, y_val     = X_all[train_end:val_end], y_all[train_end:val_end]
X_test, y_test   = X_all[val_end:], y_all[val_end:]

# =========================
# 4) Build sequences
# =========================
def make_sequences(X, y, lookback: int):
    Xs, ys = [], []
    for i in range(lookback, len(X)):
        Xs.append(X[i-lookback:i])
        ys.append(y[i])
    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)

Xtr, ytr = make_sequences(X_train, y_train, LOOKBACK)
Xva, yva = make_sequences(X_val, y_val, LOOKBACK)
Xte, yte = make_sequences(X_test, y_test, LOOKBACK)

print("Train:", Xtr.shape, ytr.shape)
print("Val:  ", Xva.shape, yva.shape)
print("Test: ", Xte.shape, yte.shape)

# =========================
# 5) Weighted loss (to chase peaks)
# =========================
def weighted_mse_loss(k: float):
    def loss(y_true, y_pred):
        w = 1.0 + k * y_true
        return tf.reduce_mean(w * tf.square(y_true - y_pred))
    return loss

def weighted_huber_loss(k: float, delta: float):
    def loss(y_true, y_pred):
        w = 1.0 + k * y_true
        err = y_true - y_pred
        abs_err = tf.abs(err)
        quad = tf.minimum(abs_err, delta)
        lin  = abs_err - quad
        huber = 0.5 * tf.square(quad) + delta * lin
        return tf.reduce_mean(w * huber)
    return loss

def get_loss(mode: str):
    mode = mode.lower()
    if mode == "weighted_mse":
        return weighted_mse_loss(WEIGHT_K)
    if mode == "weighted_huber":
        return weighted_huber_loss(WEIGHT_K, HUBER_DELTA)
    # fallback (opsional)
    return tf.keras.losses.LogCosh()

# =========================
# 6) Model definition (Pyramid LSTM + lower dropout)
# =========================
def build_model(n_features: int, lookback: int):
    model = tf.keras.Sequential([
        layers.Input(shape=(lookback, n_features)),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(DROPOUT),
        layers.LSTM(32),
        layers.Dropout(DROPOUT),
        layers.Dense(16, activation="relu"),
        layers.Dense(1)
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),
        loss=get_loss(LOSS_MODE),
        metrics=[tf.keras.metrics.MAE, tf.keras.metrics.RootMeanSquaredError()]
    )
    return model

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=15, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=6, min_lr=1e-5)
]

# =========================
# 7) Train + Evaluate (single model)
# =========================
def evaluate_predictions(y_true, y_hat, title=""):
    mae  = mean_absolute_error(y_true, y_hat)
    rmse = np.sqrt(mean_squared_error(y_true, y_hat))
    r2   = r2_score(y_true, y_hat)
    print(f"\n=== {title} ===")
    print(f"MAE  : {mae:.4f}")
    print(f"RMSE : {rmse:.4f}")
    print(f"RÂ²   : {r2:.4f}")
    return mae, rmse, r2

set_seed(42)
model = build_model(n_features=Xtr.shape[-1], lookback=LOOKBACK)

history = model.fit(
    Xtr, ytr,
    validation_data=(Xva, yva),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

y_pred = model.predict(Xte).squeeze()
evaluate_predictions(yte, y_pred, title=f"Single model ({LOSS_MODE}, dropout={DROPOUT}, k={WEIGHT_K})")

# =========================
# 8) OPTIONAL: Ensemble 3 seeds
# =========================
if DO_ENSEMBLE:
    preds = []
    for s in ENSEMBLE_SEEDS:
        set_seed(s)
        m = build_model(n_features=Xtr.shape[-1], lookback=LOOKBACK)
        m.fit(
            Xtr, ytr,
            validation_data=(Xva, yva),
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            callbacks=callbacks,
            verbose=0
        )
        preds.append(m.predict(Xte).squeeze())

    y_pred_ens = np.mean(np.stack(preds, axis=0), axis=0)
    evaluate_predictions(yte, y_pred_ens, title=f"Ensemble ({len(ENSEMBLE_SEEDS)} models)")



    # =========================
# Extra feature engineering (anti-leak)
# =========================
df = pd.read_csv(CSV_PATH)
df["TANGGAL"] = pd.to_datetime(df["TANGGAL"])
df = df.sort_values("TANGGAL").reset_index(drop=True)

# Target: 30-day rolling mean (label)
df["RR_30d_mean"] = df["RR"].rolling(30).mean()

# Seasonal encoding: month
df["month"] = df["TANGGAL"].dt.month
df["month_sin"] = np.sin(2*np.pi*df["month"]/12)
df["month_cos"] = np.cos(2*np.pi*df["month"]/12)

# Rain lags (existing)
df["RR_lag30"] = df["RR"].shift(30)
df["RR_lag60"] = df["RR"].shift(60)
df["RR_lag90"] = df["RR"].shift(90)

# Rain short lags (NEW)
for L in [7, 14, 21, 28]:
    df[f"RR_lag{L}"] = df["RR"].shift(L)

# Rolling features (NEW) - shift 1 day to avoid leakage
rr_shift1 = df["RR"].shift(1)
df["RR_roll7_mean_s1"]  = rr_shift1.rolling(7).mean()
df["RR_roll14_mean_s1"] = rr_shift1.rolling(14).mean()
df["RR_roll30_mean_s1"] = rr_shift1.rolling(30).mean()

# Wind direction cyclic encoding (NEW)
def add_dir_cyc(df, col):
    rad = 2*np.pi*df[col]/360.0
    df[f"{col}_sin"] = np.sin(rad)
    df[f"{col}_cos"] = np.cos(rad)
    return df

df = add_dir_cyc(df, "DDD_X")
df = add_dir_cyc(df, "DDD_CAR")

# Drop NA from rolling/lag/target
df = df.dropna().reset_index(drop=True)


# =========================
# Features
# =========================
# Base meteo features
base_meteo = ["TN","TX","TAVG","RH_AVG","SS","FF_X","FF_AVG"]

# Replace direction raw with cyclic
wind_cyc = ["DDD_X_sin","DDD_X_cos","DDD_CAR_sin","DDD_CAR_cos"]

# Rain memory features
rain_feats = [
    "RR_lag7","RR_lag14","RR_lag21","RR_lag28",
    "RR_lag30","RR_lag60","RR_lag90",
    "RR_roll7_mean_s1","RR_roll14_mean_s1","RR_roll30_mean_s1"
]

seasonal = ["month_sin","month_cos"]

# LSTM features (sequence)
feature_cols_lstm = base_meteo + wind_cyc + seasonal + rain_feats

# XGB features (tabular)
feature_cols_xgb = base_meteo + wind_cyc + seasonal + rain_feats

target_col = "RR_30d_mean"

X_all_lstm = df[feature_cols_lstm].values.astype(np.float32)
X_all_xgb  = df[feature_cols_xgb].values.astype(np.float32)
y_all      = df[target_col].values.astype(np.float32)



n = len(df)
train_end = int(n * 0.70)
val_end   = int(n * 0.85)

Xl_train, yl_train = X_all_lstm[:train_end], y_all[:train_end]
Xl_val,   yl_val   = X_all_lstm[train_end:val_end], y_all[train_end:val_end]
Xl_test,  yl_test  = X_all_lstm[val_end:], y_all[val_end:]

Xx_train, yx_train = X_all_xgb[:train_end], y_all[:train_end]
Xx_val,   yx_val   = X_all_xgb[train_end:val_end], y_all[train_end:val_end]
Xx_test,  yx_test  = X_all_xgb[val_end:], y_all[val_end:]

def make_sequences(X, y, lookback: int):
    Xs, ys = [], []
    for i in range(lookback, len(X)):
        Xs.append(X[i-lookback:i])
        ys.append(y[i])
    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)

Xtr, ytr = make_sequences(Xl_train, yl_train, LOOKBACK)
Xva, yva = make_sequences(Xl_val,   yl_val,   LOOKBACK)
Xte, yte = make_sequences(Xl_test,  yl_test,  LOOKBACK)

print("Train:", Xtr.shape, ytr.shape)
print("Val:  ", Xva.shape, yva.shape)
print("Test: ", Xte.shape, yte.shape)


# =========================
# Train LSTM ensemble and get predictions (aligned)
# =========================
def build_model(n_features: int, lookback: int):
    model = tf.keras.Sequential([
        layers.Input(shape=(lookback, n_features)),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(DROPOUT),
        layers.LSTM(32),
        layers.Dropout(DROPOUT),
        layers.Dense(16, activation="relu"),
        layers.Dense(1)
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),
        loss=get_loss(LOSS_MODE),
        metrics=[tf.keras.metrics.MAE, tf.keras.metrics.RootMeanSquaredError()]
    )
    return model

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=15, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=6, min_lr=1e-5)
]

def fit_lstm_and_predict(seed):
    set_seed(seed)
    m = build_model(n_features=Xtr.shape[-1], lookback=LOOKBACK)
    m.fit(Xtr, ytr, validation_data=(Xva, yva),
          epochs=EPOCHS, batch_size=BATCH_SIZE,
          callbacks=callbacks, verbose=0)
    # preds aligned with sequence outputs
    p_tr = m.predict(Xtr).squeeze()
    p_va = m.predict(Xva).squeeze()
    p_te = m.predict(Xte).squeeze()
    return p_tr, p_va, p_te

preds_tr, preds_va, preds_te = [], [], []
for s in ENSEMBLE_SEEDS:
    p_tr, p_va, p_te = fit_lstm_and_predict(s)
    preds_tr.append(p_tr); preds_va.append(p_va); preds_te.append(p_te)

yhat_lstm_tr = np.mean(np.stack(preds_tr, axis=0), axis=0)
yhat_lstm_va = np.mean(np.stack(preds_va, axis=0), axis=0)
yhat_lstm_te = np.mean(np.stack(preds_te, axis=0), axis=0)



# =========================
# XGB Residual dataset (aligned)
# =========================
y_train_al = yl_train[LOOKBACK:]
X_train_al = Xx_train[LOOKBACK:]
resid_train = y_train_al - yhat_lstm_tr

y_val_al = yl_val[LOOKBACK:]
X_val_al = Xx_val[LOOKBACK:]
resid_val = y_val_al - yhat_lstm_va

y_test_al = yl_test[LOOKBACK:]
X_test_al = Xx_test[LOOKBACK:]

# Peak weighting (threshold-based)
q = 0.90
thr = np.quantile(y_train_al, q)
K = 6.0  # can tune

w_train = np.where(y_train_al > thr, 1.0 + K, 1.0)
w_val   = np.where(y_val_al   > thr, 1.0 + K, 1.0)




# =========================
# Train XGBoost on residuals
# =========================
from xgboost import XGBRegressor

xgb = XGBRegressor(
    n_estimators=3000,
    learning_rate=0.03,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=5,
    reg_alpha=0.0,
    reg_lambda=1.0,
    objective="reg:pseudohubererror",
    random_state=42,
)

xgb.fit(
    X_train_al, resid_train,
    sample_weight=w_train,
    eval_set=[(X_val_al, resid_val)],
    sample_weight_eval_set=[w_val],
    verbose=False,
)

resid_hat_te = xgb.predict(X_test_al)


# =========================
# Final prediction: LSTM + XGB residual
# =========================
yhat_final = yhat_lstm_te + resid_hat_te

# Evaluate
evaluate_predictions(y_test_al, yhat_lstm_te, title="LSTM ensemble (aligned test)")
evaluate_predictions(y_test_al, yhat_final,   title="Hybrid Residual (LSTM + XGB)")

# Peak-only evaluation (top 10% by train threshold)
mask_peak = y_test_al > thr
evaluate_predictions(y_test_al[mask_peak], yhat_lstm_te[mask_peak], title="LSTM peak-only (test > P90 train)")
evaluate_predictions(y_test_al[mask_peak], yhat_final[mask_peak],   title="Hybrid peak-only (test > P90 train)")


